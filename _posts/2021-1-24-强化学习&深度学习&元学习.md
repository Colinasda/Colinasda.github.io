---
layout:     post
title:      强化学习&深度学习&元学习
subtitle:   三种学习方式的关系
date:       2021-1-24
author:     Zhao Zihao
header-img: img/post-bg-os-metro.jpg
catalog: false
tags:
    - 机器学习
---

#### 强化学习（Reinforcement Learning）

详细介绍：[https://zhuanlan.zhihu.com/p/25319023](https://zhuanlan.zhihu.com/p/25319023)

![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmz2v7kjguj30n90fk78l.jpg)

**Environment & Agent**

Environment指的是外部环境，在游戏中就是游戏的环境。Agent指的是智能体，指的就是你写的算法，在游戏中就是玩家，智能体通过一套策略输出一个行为（Action）作用到环境，环境则反馈状态值，也就是Observation，和奖励值Reward到智能体，同时环境会转移到下一个状态。如此不断循环，最终找到一个最优的策略，使得智能体可以尽可能多的获得来自环境的奖励。

![](https://tva1.sinaimg.cn/large/008eGmZEly1gmpj3o14cej30b0068dgk.jpg)

从本质上讲，强化学习是一个**闭环控制**问题。

***

#### 深度学习&强化学习&元学习

统计学是人工智能开始发展的一个基础，古老的人们从大量的数据中发现存在的规律，在以统计学为基础的 **机器学习**(machine learning)时代，复杂一点的分类问题效果就不好了, **深度学习**(Deep Learning)的出现基本上解决了一对一映射的问题，比如说图像分类，一个输入对一个输出，因此出现了AlexNet这样的里程碑式的成果。但如果输出对下一个输入还有影响呢？也就是序列决策(sequential decision making) 的问题，单一的深度学习就解决不了了。

**强化学习**(Reinforcement Learning)的出现让该问题得到了新的发展，深度强化学习（Deep Learning + Reinforcement Learning = Deep Reinforcement Learning）通过使用神经网络对强化学习的学习数据利用神经网络进行逼近得到了快速的发展，让序列决策初步取得成效，最典型的例子就是Ｇoogle DeepMind公司的AlphaGo。

但是，新的问题又出来了，**深度强化学习** 过度依赖于巨量的训练，并且需要精确的Reward，对于现实世界的很多问题，比如机器人学习，没有好的reward，也没办法无限量训练，怎么办？这就需要能够快速学习。


相比之下，人类能够更快、更有效地学习新的概念和技能。只看过几次猫和鸟的孩子可以很快分辨出来。知道如何骑自行车的人可能很快就会发现骑摩托车的方式。那么是否有可能设计出具有类似属性的机器学习模型 ，答案是“有的”，也就是**元学习(Meta-Learning)**
